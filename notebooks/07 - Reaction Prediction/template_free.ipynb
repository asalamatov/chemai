{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4b4d368-3e7e-461d-bd7f-1c81a791e14d",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/schwallergroup/ai4chem_course/blob/main/notebooks/07%20-%20Reaction%20Prediction/template_free.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e284fe-c36c-47cc-b8d6-e716043b2c0c",
   "metadata": {},
   "source": [
    "# Week 7: reaction prediction with Molecular Transformer\n",
    "\n",
    "We have talked about different reaction prediction methods. Template-based approaches use reaction templates to select the most likely reaction outcome. However, these models are limited by the dataset (the model can just learn reaction classes, and is not able to get deeper features), quality and quantity of the templates (you can only predict reactions that are included in your templates, and the model will be heavily dependent on the quality of the atom mapping), and they cannot predict selectivity.\n",
    "\n",
    "On the other hand, template-free models can overcome many of these limitations. Today, we will use a language model, the Molecular Transformer, to do chemical reaction prediction. You can check the original paper [here](https://pubs.acs.org/doi/full/10.1021/acscentsci.9b00576). \n",
    "\n",
    "This model is based on the Transformer architecture, which is behind some of the most remarkable AI models of the last years (eg. [AlphaFold2](https://www.nature.com/articles/s41586-021-03819-2) or [ChatGPT](https://chat.openai.com/)). Basically, we will treat reaction prediction as a machine translation model, where the model will learn the \"grammar of the reactions\". Some of the main advantages of the model are \n",
    "\n",
    "![](https://pubs.acs.org/cms/10.1021/acscentsci.9b00576/asset/images/medium/oc9b00576_0009.gif)\n",
    "</br><left>Figure 1: SMILES-to-SMILES translation with the Molecular Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20519980",
   "metadata": {},
   "source": [
    "# 0. Relevant packages\n",
    "\n",
    "### OpenNMT \n",
    "OpenNMT is an open-source library for translation tasks. We will use it to create and train our model as it is a flexible and easy to use framework. You can check the documentation [here](https://opennmt.net/OpenNMT-py/index.html).\n",
    "\n",
    "We will also use rdkit and other common auxiliary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de815270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if you have a notebook with a GPU\n",
    "import torch\n",
    "torch.cuda.is_available()\n",
    "# If False, Go to Menu > Runtime > Change runtime. Hardware accelerator -> GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac68d101-45a4-4a58-b108-6e532d295006",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install rdkit-pypi==2022.3.1\n",
    "!pip install pip install OpenNMT-py==2.2.0\n",
    "!wget https://raw.githubusercontent.com/schwallergroup/ai4chem_course/main/notebooks/07%20-%20Reaction%20Prediction/utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc57062-f29a-4317-b622-d60df8fb7bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import gdown\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "from rdkit import Chem\n",
    "\n",
    "# to display molecules\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "from rdkit.Chem import Draw\n",
    "IPythonConsole.ipython_useSVG=True\n",
    "\n",
    "# disable RDKit warnings\n",
    "from rdkit import RDLogger\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "\n",
    "from utils import download_data, canonicalize_smiles, evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18f3eea-a030-4d6c-880d-fa9fd17b362f",
   "metadata": {},
   "source": [
    "# Download data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328087c2",
   "metadata": {},
   "source": [
    "Firstly, we will download the data for training. We will use [USPTO](https://figshare.com/articles/dataset/Chemical_reactions_from_US_patents_1976-Sep2016_/5104873), a widely used dataset containing reaction SMILES extracted from the US list of patents from 1976 to 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7195a5c3-8953-4cca-a150-daec5230f20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df, test_df = download_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559456c8",
   "metadata": {},
   "source": [
    "After running this cell you should get these sets (check the USPTO_480 folder in your Colab directory):\n",
    "\n",
    "    - Train: we will use these reaction SMILES to train the model\n",
    "    - Validation: used for checking the learning progress during training and hyperparameter tuning\n",
    "    - Test: we will use these data only once we get the final model to test its performance\n",
    "\n",
    "Each set contains two `.txt` files, one containing the precursors (reactants, src) and another containing the targets (products, tgt). Remember that our task can be seen as a translation between two sequences, reactants and products SMILES in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca37560-5b6b-4679-8cc6-8bfcdc556b19",
   "metadata": {},
   "source": [
    "# Tokenization <a class=\"anchor\" id=\"third-bullet\"></a>\n",
    "\n",
    "We already mentioned tokenization in the previous tutorial when training the LSTM. To be able to train a language model, we need to split the strings into tokens.\n",
    "\n",
    "We take the regex pattern introduced in the [Molecular Transformer](https://pubs.acs.org/doi/abs/10.1021/acscentsci.9b00576) paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb59cf9-b9b2-4722-aaf7-bf4ad53cb2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "SMI_REGEX_PATTERN =  r\"(\\%\\([0-9]{3}\\)|\\[[^\\]]+]|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|\\||\\(|\\)|\\.|=|#|-|\\+|\\\\|\\/|:|~|@|\\?|>>?|\\*|\\$|\\%[0-9]{2}|[0-9])\"\n",
    "\n",
    "def smiles_tokenizer(smiles):\n",
    "    smiles_regex = re.compile(SMI_REGEX_PATTERN)\n",
    "    tokens = [token for token in smiles_regex.findall(smiles)]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "\n",
    "print('Tokenizing training set')\n",
    "train_df['tokenized_precursors'] = train_df.precursors.progress_apply(lambda smi: smiles_tokenizer(smi))\n",
    "train_df['tokenized_products'] = train_df.products.progress_apply(lambda smi: smiles_tokenizer(smi))\n",
    "print('Tokenizing validation set')\n",
    "val_df['tokenized_precursors'] = val_df.precursors.progress_apply(lambda smi: smiles_tokenizer(smi))\n",
    "val_df['tokenized_products'] = val_df.products.progress_apply(lambda smi: smiles_tokenizer(smi))\n",
    "print('Tokenizing test set')\n",
    "test_df['tokenized_precursors'] = test_df.precursors.progress_apply(lambda smi: smiles_tokenizer(smi))\n",
    "test_df['tokenized_products'] = test_df.products.progress_apply(lambda smi: smiles_tokenizer(smi))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83597c51-fc70-4a25-b724-ee60048fdd79",
   "metadata": {},
   "source": [
    "## Save the preprocessed data set\n",
    "\n",
    "Don't forget to shuffle the training set before saving it. At least earlier versions of OpenNMT-py would not shuffle it during preprocessing. After that, we save the tokenized files, which are now ready to use for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b9bfed-c244-4bbd-843b-1c889bf0f7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_train_df = train_df.sample(frac=1., random_state=42)\n",
    "data_path = 'USPTO_480k_preprocessed'\n",
    "\n",
    "os.makedirs(data_path, exist_ok=True)\n",
    "with open(os.path.join(data_path, 'precursors-train.txt'), 'w') as f:\n",
    "    f.write('\\n'.join(shuffled_train_df.tokenized_precursors.values))\n",
    "with open(os.path.join(data_path, 'products-train.txt'), 'w') as f:\n",
    "    f.write('\\n'.join(shuffled_train_df.tokenized_products.values))\n",
    "\n",
    "with open(os.path.join(data_path, 'precursors-val.txt'), 'w') as f:\n",
    "    f.write('\\n'.join(val_df.tokenized_precursors.values))\n",
    "with open(os.path.join(data_path, 'products-val.txt'), 'w') as f:\n",
    "    f.write('\\n'.join(val_df.tokenized_products.values))\n",
    "    \n",
    "with open(os.path.join(data_path, 'precursors-test.txt'), 'w') as f:\n",
    "    f.write('\\n'.join(test_df.tokenized_precursors.values))\n",
    "with open(os.path.join(data_path, 'products-test.txt'), 'w') as f:\n",
    "    f.write('\\n'.join(test_df.tokenized_products.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3798bdbe-3ed0-4522-af00-f10f4e578269",
   "metadata": {},
   "source": [
    "# Building the vocab <a class=\"anchor\" id=\"fourth-bullet\"></a>\n",
    "\n",
    "The first step for the [OpenNMT-py pipeline](https://opennmt.net/OpenNMT-py/quickstart.html) is to build the vocabulary.\n",
    "\n",
    "![](https://camo.githubusercontent.com/69fb11841ce1abd51a3fd7f3ed4b424857029ce123521cc301eb48a1e22bee2f/687474703a2f2f6f70656e6e6d742e6769746875622e696f2f73696d706c652d6174746e2e706e67)\n",
    "</br><left>Figure 2: In contrast to a neural machine translation model for human language, we will use an atom-wise vocabulary. \n",
    "\n",
    "\n",
    "Please note:\n",
    "- Typical sequence pairs in machine translation are much shorter than the ones you encounter in chemical reaction prediction. Hence, set a `src_seq_length` and `tgt_seq_length` that is much higher than the maximum you would expect to include all reactions (in this case we set a value of 1000).\n",
    "- With `n_sample` set to `-1` we include the whole dataset.\n",
    "\n",
    "The paths to the training and validation datasets are defined in the `run_config.yaml`:\n",
    "\n",
    "```yaml\n",
    "# Here you can check the documentation to know better how this file works.\n",
    "# https://opennmt.net/OpenNMT-py/quickstart.html\n",
    "# Examples in https://github.com/OpenNMT/OpenNMT-py/tree/master/config\n",
    "\n",
    "## Where the samples will be written\n",
    "save_data: example_run\n",
    "## Where the vocab(s) will be written\n",
    "src_vocab: example_run/uspto.vocab.src\n",
    "tgt_vocab: example_run/uspto.vocab.src\n",
    "# Prevent overwriting existing files in the folder\n",
    "overwrite: true\n",
    "share_vocab: true\n",
    "\n",
    "# Corpus opts:\n",
    "data:\n",
    "    corpus-1:\n",
    "        path_src: USPTO_480k_preprocessed/precursors-train.txt\n",
    "        path_tgt: USPTO_480k_preprocessed/products-train.txt\n",
    "    valid:\n",
    "        path_src: USPTO_480k_preprocessed/precursors-val.txt\n",
    "        path_tgt: USPTO_480k_preprocessed/products-val.txt\n",
    "```\n",
    "\n",
    "As the source (precusors) and the target (products) are represented as SMILES and consist of the same tokens, we share the vocabulary between source and target (`share_vocab: true`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aeb29f1-fd45-4970-bd14-54d2648c7de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_url = 'https://raw.githubusercontent.com/schwallergroup/dmds_language_models_for_reactions/main/example_run/run_config.yaml'\n",
    "config_folder = 'example_run'\n",
    "config_name = 'run_config.yaml'\n",
    "\n",
    "os.makedirs(config_folder, exist_ok=True)\n",
    "target_path = os.path.join(config_folder, config_name)\n",
    "if not os.path.exists(target_path):\n",
    "    gdown.download(config_url, target_path, quiet=False)\n",
    "else:\n",
    "    print(f\"{target_path} already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675243f6",
   "metadata": {},
   "source": [
    "Now we can run this command to build our vocabulary file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036933c8-6af8-45c2-ae83-698901e99f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "! onmt_build_vocab -config example_run/run_config.yaml \\\n",
    "    -src_seq_length 1000 -tgt_seq_length 1000 \\\n",
    "    -src_vocab_size 1000 -tgt_vocab_size 1000 \\\n",
    "    -n_sample -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f097cdd",
   "metadata": {},
   "source": [
    "You can check how the `uspto.vocab.src` in the `example_run` folder to see how the vocabulary looks like."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7869e844-0951-4fc4-9fad-a73796ce98e4",
   "metadata": {},
   "source": [
    "# Training the Molecular Transformer <a class=\"anchor\" id=\"fifth-bullet\"></a>\n",
    "\n",
    "If you look at the `run_config.yaml`, you will see that we have defined some of the training parameters (but not yet the hyperparameters of the model).\n",
    "\n",
    "```yaml\n",
    "# Train on a single GPU\n",
    "world_size: 1\n",
    "gpu_ranks: [0]\n",
    "\n",
    "# Where to save the checkpoints\n",
    "save_model: example_run/model\n",
    "save_checkpoint_steps: 5000\n",
    "keep_checkpoint: 3\n",
    "train_steps: 400000\n",
    "valid_steps: 10000\n",
    "report_every: 100\n",
    "\n",
    "tensorboard: true\n",
    "tensorboard_log_dir: log_dir\n",
    "```\n",
    "\n",
    "The Transformer architecture was published in the [Attention is all you need](https://proceedings.neurips.cc/paper/7181-attention-is-all-you-need) paper by Vaswani et al. (NeurIPS, 2017). The model sizes (65 to 212M parameters) in that paper were larger than what we use for reaction prediction (20M parameters). \n",
    "\n",
    "![](https://raw.githubusercontent.com/nlp-with-transformers/notebooks/main/images/chapter01_self-attention.png)\n",
    "</br><left>Figure 3: Transformer model (source: https://github.com/nlp-with-transformers). </left>\n",
    "\n",
    "Illustrated transformer blogposts:\n",
    "- https://nlp.seas.harvard.edu/2018/04/03/attention.html\n",
    "- https://jalammar.github.io/illustrated-transformer/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3ffc8a",
   "metadata": {},
   "source": [
    "The cell below contains the command used to train the model. Here you can set all your hyperparameters. In this case, we will use the values that were published in this [paper](https://www.nature.com/articles/s41467-020-18671-7), where the Molecular Transformer was trained to predict carbohydrate reactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91629dc0-409f-4270-9286-1cd1c6bfd936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters from https://github.com/rxn4chemistry/OpenNMT-py/tree/carbohydrate_transformer\n",
    "!onmt_train -config example_run/run_config.yaml \\\n",
    "        -seed 42 -gpu_ranks 0  \\\n",
    "        -param_init 0 \\\n",
    "        -param_init_glorot -max_generator_batches 32 \\\n",
    "        -batch_type tokens -batch_size 6144\\\n",
    "        -normalization tokens -max_grad_norm 0  -accum_count 4 \\\n",
    "        -optim adam -adam_beta1 0.9 -adam_beta2 0.998 -decay_method noam  \\\n",
    "        -warmup_steps 8000 -learning_rate 2 -label_smoothing 0.0 \\\n",
    "        -layers 4 -rnn_size  384 -word_vec_size 384 \\\n",
    "        -encoder_type transformer -decoder_type transformer \\\n",
    "        -dropout 0.1 -position_encoding -share_embeddings  \\\n",
    "        -global_attention general -global_attention_function softmax \\\n",
    "        -self_attn_type scaled-dot -heads 8 -transformer_ff 2048 \\\n",
    "        -tensorboard True -tensorboard_log_dir log_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade2ad81-dff7-43bf-ade7-c2e07d708845",
   "metadata": {},
   "source": [
    "The training can take more than `24 hours` on a single GPU! Hence, stop the previous cell, we will directly download the trained model :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2301f375-31b3-4887-a7bc-1df88742be31",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model_url = 'https://drive.google.com/uc?id=1ywJCJHunoPTB5wr6KdZ8aLv7tMFMBHNy'\n",
    "model_folder = 'models'\n",
    "model_name = 'USPTO480k_model_step_400000.pt'\n",
    "\n",
    "os.makedirs(model_folder, exist_ok=True)\n",
    "target_path = os.path.join(model_folder, model_name)\n",
    "if not os.path.exists(target_path):\n",
    "    gdown.download(trained_model_url, target_path, quiet=False)\n",
    "else:\n",
    "    print(f\"{target_path} already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699d8423-883e-4b12-bd77-07cc7a8e501c",
   "metadata": {},
   "source": [
    "# Evaluating the model \n",
    "\n",
    "We'll use a pre-made script for this, don't worry about it now. Basically we want to check what percentage of the predictions are correct (we compute the accuracy of the model). *This cell takes around 20 minutes to execute, as we are using the model to predict nearly 40k reactions*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde99d02-d4f2-4173-ad18-8d76f1effbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!onmt_translate -model models/USPTO480k_model_step_400000.pt -gpu 0 \\\n",
    "    --src USPTO_480k_preprocessed/precursors-val.txt \\\n",
    "    --tgt USPTO_480k_preprocessed/products-val.txt \\\n",
    "    --output models/USPTO480k_model_step_400000_val_predictions.txt \\\n",
    "    --n_best 5 --beam_size 10 --max_length 300 --batch_size 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b7c2a2",
   "metadata": {},
   "source": [
    "For each reaction, we obtain 5 predictions that can be used to compute the accuracy. The `top-n` accuracy is the accuracy that includes the best n predictions (for example, top-3 accuracy will consider one prediction as true if the true product matches any of the best 3 predictions). Now run this to compute the accuracy of the model. What do these numbers tell you?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04537213-80e8-4a4d-95a2-9a02cb8acfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(n_best=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef64963c-711d-4003-beb4-60b3ce9dc64d",
   "metadata": {},
   "source": [
    "## Exercise: Test the limits of the Molecular Transformer!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67385cb0",
   "metadata": {},
   "source": [
    "Now it's your turn! Try to make some predictions using the Molecular Transformer and check its limitations. You can try to explore reactions with challenging stereochemistry or regioselectivity, and see if the model is able to correctly get them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257c15b7-e4dd-45a2-875a-02b3ac27d985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's an example\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "def predict_reaction(precursors):\n",
    "    \"\"\" predict product of one reaction from its precursors SMILES list\n",
    "    \"\"\"\n",
    "\n",
    "    smi_pres = '.'.join(precursors)\n",
    "    smi_pres = smiles_tokenizer(smi_pres)\n",
    "\n",
    "    with open('precursors-try.txt', 'w') as f:\n",
    "        f.write(f'{smi_pres}\\n')\n",
    "\n",
    "    os.system(\"rm preds.txt\")\n",
    "    os.system(\"onmt_translate -model models/USPTO480k_model_step_400000.pt -gpu 0 \\\n",
    "               --src precursors-try.txt --output preds.txt \\\n",
    "               --n_best 1 --beam_size 5 --max_length 300 --batch_size 64\")\n",
    "\n",
    "    with open('preds.txt', 'r') as f:\n",
    "        pred_smi = f.readline().strip()\n",
    "        pred_smi = pred_smi.replace(\" \", \"\")\n",
    "\n",
    "    # print result\n",
    "    print(f\"The SMILES of the predicted product is: {pred_smi}\")\n",
    "    # Use RDKit to visualize the reactants and product\n",
    "    # precursors\n",
    "    print(\"\\n\\nVisualization of the reaction:\\n\")\n",
    "    print(\"Precursors:\")\n",
    "    precursors_mols = [Chem.MolFromSmiles(smi) for smi in precursors]\n",
    "    [display(mol) for mol in precursors_mols]\n",
    "    print(\"Product:\")\n",
    "    product_mol = Chem.MolFromSmiles(pred_smi)\n",
    "    display(product_mol)\n",
    "\n",
    "    return pred_smi\n",
    "\n",
    "# Write the SMILES of all precursors for a reaction you want to predict into a list\n",
    "precursors_smis = [\n",
    "    \"COC(=O)c1cc2c3cccc(Cl)c3n(C)c2s1\",\n",
    "    \"[K+].[OH-]\",\n",
    "]\n",
    "\n",
    "pred_smi = predict_reaction(precursors_smis)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "1be16fbddf550357e4e46540ee01bc6d12a48d7bc56fc8427cd30121d5943dc8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
