{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e5e768a-9b68-48c6-9af0-9281a9762d6f",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/schwallergroup/ai4chem_course/blob/main/notebooks/03%20-%20Intro%20to%20Deep%20Learning/01_intro_to_dl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367bae14-53f5-4adf-a28c-d8658dd8cd85",
   "metadata": {},
   "source": [
    "# Week 3 tutorial 1 - AI 4 Chemistry\n",
    "\n",
    "## Table of content\n",
    "\n",
    "1. Supervised deep learning.\n",
    "2. Neural Networks.\n",
    "3. Creating a deep learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b17b43",
   "metadata": {},
   "source": [
    "# 0. Relevant packages\n",
    "\n",
    "### Pytorch\n",
    "Based on the Torch library, PyTorch is one of the most popular deep learning frameworks for machine learning practitioners. We will learn to use PyTorch to do deep learning work. You can also browse the PyTorch [tutorials](https://pytorch.org/tutorials/) and [docs](https://pytorch.org/docs/stable/index.html) for additional details.\n",
    "\n",
    "### Pytorch Lightning\n",
    "PyTorch Lightning is the deep learning framework for professional AI researchers and machine learning engineers who need maximal flexibility without sacrificing performance at scale. You can also browse its [documentation](https://pytorch-lightning.readthedocs.io/en/stable/) for additional details.\n",
    "\n",
    "### Weights & Biases (W&B)\n",
    "Weights & Biases is the machine learning platform for developers to build better models faster. Use W&B's lightweight, interoperable tools to quickly track experiments, version and iterate on datasets, evaluate model performance, reproduce models, visualize results and spot regressions, and share findings with colleagues. You can also browse its [documentation](https://docs.wandb.ai/) for additional details.\n",
    "\n",
    "## Exercise: Create a W&B account.\n",
    "#### Go to [W&B](https://wandb.ai/site) and create an account. We will be using this platform to track our experiments!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5ebe28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all libraries\n",
    "! pip install pytorch-lightning wandb rdkit ogb deepchem torch\n",
    "\n",
    "# Download all data\n",
    "! mkdir data/\n",
    "! wget https://raw.githubusercontent.com/schwallergroup/ai4chem_course/main/notebooks/02%20-%20Supervised%20Learning/data/esol.csv -O data/esol.csv\n",
    "! wget https://raw.githubusercontent.com/schwallergroup/ai4chem_course/main/notebooks/03%20-%20Intro%20to%20Deep%20Learning/esol_utils.py -O esol_utils.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f9fe6b",
   "metadata": {},
   "source": [
    "Set a random seed to ensure repeatability of experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e833c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Random Seeds and Reproducibility\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5e52b4-30d8-4940-9d54-e71e5ece96ce",
   "metadata": {},
   "source": [
    "# 1. Supervised Deep Learning\n",
    "\n",
    "From last session we should already be familiar with supervised learning: is a type of machine learning that involves training a model on a labeled dataset to learn the relationships between input and output data.\n",
    "\n",
    "The models we saw so far are fairly easy and work well in some scenarios, but sometimes it's not enough. What to do in these cases?\n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"img/deeper_meme.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "### Deep Learning\n",
    "Deep learning is a subset of machine learning that involves training artificial neural networks to learn from data. Unlike traditional machine learning algorithms, which often rely on hand-crafted features and linear models, deep learning algorithms can automatically learn features and hierarchies of representations from raw data. This allows deep learning models to achieve state-of-the-art performance on a wide range of tasks in chemistry, like molecular property prediction, reaction prediction and retrosynthesis, among others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fff8f48-392d-4a61-9480-7751414bf029",
   "metadata": {},
   "source": [
    "#### Data: Let's go back to the [ESOL dataset](https://pubs.acs.org/doi/10.1021/ci034243x) from last week.\n",
    "We will use this so we can compare our results with the previous models. We'll reuse last week's code for  data loading and preprocessing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3cc74b-ca98-4db2-bd26-b6579bd01c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from esol_utils import load_esol_data\n",
    "(X_train, X_valid, X_test, y_train, y_valid, y_test, scaler) = load_esol_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444e20ea-8613-4580-976f-433b898efbf5",
   "metadata": {},
   "source": [
    "## 2. Neural Networks\n",
    "\n",
    "Neural Networks are a type of machine learning model that is designed to simulate the behavior of the human brain.\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"img/nn_image.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "\\\n",
    "They consist of layers of interconnected nodes, and each node applies a `linear function` to its inputs. Non-linear activation functions are used to introduce `non-linearity` into the model, allowing it to learn more complex patterns in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef51c06-44a3-4581-b735-e08766108e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import wandb\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader\n",
    "from pytorch_lightning.loggers import WandbLogger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1749f9a1-4a86-413d-b4ce-7428856c5009",
   "metadata": {},
   "source": [
    "## 3. Creating a deep learning model.\n",
    "\n",
    "Creating DL models is fairly easy nowadays, specially thanks to libraries like [Pytorch Lightning](https://pytorch-lightning.readthedocs.io/en/stable/index.html). They do most of the work for you, but they still alow you to have a lot of control over your models.\n",
    "\n",
    "To use Pytorch Lightning, we first need to know about **classes**.\n",
    "\n",
    "\n",
    "> Think of a class as a template or a set of instructions for creating objects with specific properties and behaviors. These objects are called instances of the class.\n",
    "\n",
    "\\\n",
    "For example, let's say you want to make a program to represent dogs.\n",
    "\n",
    "```python\n",
    "class Dog:\n",
    "    def __init__(self, name, color):\n",
    "        self.name = name\n",
    "        self.color = color\n",
    "        \n",
    "    def say_your_name(self):\n",
    "        print(f\"My name is {self.name}\")\n",
    "       \n",
    "```\n",
    "\n",
    "In this example, a dog has two attributes: `name` and `color`. It also has a method: `say_your_name`.\n",
    "\n",
    "Now we can create as many dogs as we want! For example\n",
    "\n",
    "```python\n",
    "lassie = Dog(name = \"Lassie\", color = \"White\")\n",
    "pluto = Dog(name = \"Pluto\", color = \"Yellow\")\n",
    "```\n",
    "\n",
    "And we can access their methods as follows:\n",
    "\n",
    "```python\n",
    "pluto.say_your_name()   # Output: \"My name is Pluto\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "<font color=\"#4caf50\" size=4>\n",
    "Now let's define a NeuralNetwork class.\n",
    "</font>\n",
    "\n",
    "- What is each part? \n",
    "    - `__init__` is where we specify the model architecture, \n",
    "       There are loads of layers (model parts) you can use,\n",
    "       and it's all defined here.\n",
    "        \n",
    "    - `training step` is one of our model's methods. It updates the model paramters using an optimizer.\n",
    "    \n",
    "    - `configure_optimizers`, well, configures the optimizers ðŸ˜….\\\n",
    "       Here we define what optimizer to use, including learning rate.\n",
    "    \n",
    "    - `forward` specifices what the model should do when an input is given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2457d73-1d02-437d-a5ef-a912fa862ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(pl.LightningModule):\n",
    "    def __init__(self, input_sz, hidden_sz, train_data, valid_data, test_data, batch_size=254, lr=1e-3):\n",
    "        super().__init__()\n",
    "        self.lr = lr\n",
    "        self.train_data = train_data\n",
    "        self.valid_data = valid_data\n",
    "        self.test_data = test_data\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Define all the components\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_sz, hidden_sz),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_sz, hidden_sz),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_sz, 1)\n",
    "        )\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Here we define the train loop.\n",
    "        x, y = batch\n",
    "        z = self.model(x)\n",
    "        loss = F.mse_loss(z, y)\n",
    "        self.log(\"Train loss\", loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # Define validation step. At the end of every epoch, this will be executed\n",
    "        x, y = batch\n",
    "        z = self.model(x)\n",
    "        loss = F.mse_loss(z, y)  # report MSE\n",
    "        self.log(\"Valid MSE\", loss)\n",
    "        \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # What to do in test\n",
    "        x, y = batch\n",
    "        z = self.model(x)\n",
    "        loss = F.mse_loss(z, y)  # report MSE\n",
    "        self.log(\"Test MSE\", loss)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Here we configure the optimization algorithm.\n",
    "        optimizer = torch.optim.Adam(\n",
    "            self.parameters(),\n",
    "            lr=self.lr\n",
    "        )\n",
    "        return optimizer\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Here we define what the NN does with its parts\n",
    "        return self.model(x).flatten()\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_data, batch_size=self.batch_size, shuffle=True)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.valid_data, batch_size=self.batch_size, shuffle=False)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_data, batch_size=self.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed0b379-723b-482c-95b3-9423261a171d",
   "metadata": {},
   "source": [
    "### Dataset class\n",
    "\n",
    "To use Lightning, we also need to create a `Dataset` class.\\\n",
    "It looks more complicated, but it actually allows a lot of flexibility in more complex scenarios! (so don't be daunted by this ðŸ˜‰)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab601a17-f42f-4ee8-8d59-8dbdb928dd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ESOLDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        X_ = torch.as_tensor(self.X[idx].astype(np.float32))\n",
    "        y_ = torch.as_tensor(self.y[idx].astype(np.float32).reshape(-1))\n",
    "        \n",
    "        return X_, y_\n",
    "    \n",
    "train_data = ESOLDataset(X_train, y_train)\n",
    "valid_data = ESOLDataset(X_valid, y_valid)\n",
    "test_data = ESOLDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1f4682-5628-4284-815f-ebeab95d11de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This will ask you to login to your wandb account\n",
    "\n",
    "wandb.init(project=\"nn-solubility\",\n",
    "           config={\n",
    "               \"batch_size\": 32,\n",
    "               \"learning_rate\": 0.001,\n",
    "               \"hidden_size\": 512,\n",
    "               \"max_epochs\": 100\n",
    "           })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321aa13d-a145-4bf1-bd8a-72d48170bb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we create an instance of our neural network.\n",
    "# Play around with the hyperparameters!\n",
    "nn_model = NeuralNetwork(\n",
    "    input_sz = X_train.shape[1],\n",
    "    hidden_sz = wandb.config[\"hidden_size\"],\n",
    "    train_data = train_data,\n",
    "    valid_data = valid_data,\n",
    "    test_data = test_data,\n",
    "    lr = wandb.config[\"learning_rate\"],\n",
    "    batch_size=wandb.config[\"batch_size\"]\n",
    ")\n",
    "\n",
    "# Define trainer: How we want to train the model\n",
    "wandb_logger = WandbLogger()\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs = wandb.config[\"max_epochs\"],\n",
    "    logger = wandb_logger\n",
    ")\n",
    "\n",
    "# Finally! Training a model :)\n",
    "trainer.fit(\n",
    "    model=nn_model,\n",
    ")\n",
    "\n",
    "# Now run test\n",
    "results = trainer.test(ckpt_path=\"best\")\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04fe121-0d70-4e53-b2b9-87ba317f0f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test RMSE\n",
    "test_mse = results[0][\"Test MSE\"]\n",
    "test_rmse = test_mse ** 0.5\n",
    "print(f\"\\nANN model performance: RMSE on test set = {test_rmse:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4c1c9a-7867-4763-932f-9ac3da21f16e",
   "metadata": {},
   "source": [
    "# Exercise:\n",
    "\n",
    "Play with the hyperparameters, see what you get.\n",
    "\n",
    "You may play around with `hidden_sz`, `batch_sz`, `max_epochs`, `lr`,\\\n",
    "or even modify the architecture of our neural network i.e. change the number of layers, activation function, etc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30845819-7de2-407c-a368-ad9f62020428",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai4chem",
   "language": "python",
   "name": "ai4chem"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "e1b1e114f4dae097b9e32029c5d22d73dc21a5dd723446d46774bd2adced9390"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
